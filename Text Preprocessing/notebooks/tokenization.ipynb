{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32e12bf",
   "metadata": {},
   "source": [
    "# Tokenization in NLP\n",
    "\n",
    "**Tokenization** is the process of breaking down text into smaller units called **tokens**. These tokens can be words, subwords, or characters depending on the tokenization strategy.\n",
    "\n",
    "## Why is Tokenization Important?\n",
    "- It's the **first step** in most NLP pipelines\n",
    "- Helps in understanding the structure of text\n",
    "- Enables further processing like POS tagging, NER, etc.\n",
    "\n",
    "In this notebook, we'll explore tokenization using **spaCy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:04:44.352567594Z",
     "start_time": "2025-12-31T11:04:43.159214307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03338b65",
   "metadata": {},
   "source": [
    "## Basic Tokenization with spaCy\n",
    "\n",
    "Let's create a blank English model and tokenize a simple sentence. The blank model only performs tokenization without any additional NLP components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e605fad69f5dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:07:24.309892305Z",
     "start_time": "2025-12-31T11:07:24.187054217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      "over\n",
      "13\n",
      "lazy\n",
      "dogs\n",
      ",\n",
      "and\n",
      "it\n",
      "does\n",
      "n't\n",
      "stop\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumps over 13 lazy dogs, and it doesn't stop.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b25c0938018e22a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:13:00.312565554Z",
     "start_time": "2025-12-31T11:13:00.306043440Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I gave three $ to Peter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe588a",
   "metadata": {},
   "source": [
    "## Token Attributes\n",
    "\n",
    "Each token in spaCy is a `Token` object with many useful attributes. Let's explore some of them:\n",
    "- `token.text` - The original text of the token\n",
    "- `token.i` - The index of the token in the document\n",
    "- `token.is_alpha` - Is the token alphabetic?\n",
    "- `token.is_punct` - Is the token punctuation?\n",
    "- `token.like_num` - Does the token look like a number?\n",
    "- `token.is_currency` - Is the token a currency symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb447456227a8414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:13:30.437309400Z",
     "start_time": "2025-12-31T11:13:30.377491993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af599e6764c2a9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:14:56.946603695Z",
     "start_time": "2025-12-31T11:14:56.925688042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef83254c34c33e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:15:53.096588457Z",
     "start_time": "2025-12-31T11:15:53.076667398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef2bd21e00eeaf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:15:11.652143904Z",
     "start_time": "2025-12-31T11:15:11.613447748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f44ea7565c6316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:15:48.924701496Z",
     "start_time": "2025-12-31T11:15:48.902375498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc[3]\n",
    "token3.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee0f8c49d601486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:16:00.237506635Z",
     "start_time": "2025-12-31T11:16:00.204261261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5680c008f79c9e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:17:39.433753584Z",
     "start_time": "2025-12-31T11:17:39.366813282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "three ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i,\n",
    "          \"is_alpha:\", token.is_alpha,\n",
    "          \"is_punct:\", token.is_punct,\n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f63e463e5d9cc98c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:20:23.409613914Z",
     "start_time": "2025-12-31T11:20:23.352323658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72934d",
   "metadata": {},
   "source": [
    "## Practical Example: Extracting Emails from Text\n",
    "\n",
    "Let's use the `like_email` token attribute to extract all email addresses from a file. This demonstrates how tokenization combined with token attributes can be used for information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8ed1a5c70a93cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:20:48.749254639Z",
     "start_time": "2025-12-31T11:20:48.679065465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972daeb1c94437f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:22:03.913188047Z",
     "start_time": "2025-12-31T11:22:03.888753435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6f6dc7d7bb8868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:24:34.550697800Z",
     "start_time": "2025-12-31T11:24:34.491607016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "राम\n",
      "ने\n",
      "सीता\n",
      "को\n",
      "एक\n",
      "पत्र\n",
      "लिखा\n",
      "।\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "doc = nlp(\"राम ने सीता को एक पत्र लिखा।\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707fcff",
   "metadata": {},
   "source": [
    "## Multilingual Tokenization\n",
    "\n",
    "spaCy supports tokenization for many languages. Let's try tokenizing Hindi text using a blank Hindi model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ca9dff1e13f1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:26:27.120472622Z",
     "start_time": "2025-12-31T11:26:27.088088003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "token = [token.text for token in doc]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb3a67",
   "metadata": {},
   "source": [
    "## Custom Tokenization Rules\n",
    "\n",
    "Sometimes the default tokenizer doesn't handle certain words the way we want. We can add **special cases** to customize tokenization behavior.\n",
    "\n",
    "For example, \"gimme\" should be split into \"gim\" + \"me\" for better language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd722f572556b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:28:40.534073946Z",
     "start_time": "2025-12-31T11:28:40.514587751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [{ORTH: \"gim\"}, {ORTH: \"me\"}])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "token = [token.text for token in doc]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff5653072bb22ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:29:33.034152879Z",
     "start_time": "2025-12-31T11:29:32.974263489Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m doc = nlp(\u001b[33m\"\u001b[39m\u001b[33mApple is looking at buying U.K. startup for $1 billion. Hydra is a dragon of India\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/lib64/python3.12/site-packages/spacy/tokens/doc.pyx:926\u001b[39m, in \u001b[36msents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion. Hydra is a dragon of India\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e5420",
   "metadata": {},
   "source": [
    "## Sentence Tokenization (Sentence Boundary Detection)\n",
    "\n",
    "Breaking text into sentences is another form of tokenization. spaCy uses the **sentencizer** component to detect sentence boundaries.\n",
    "\n",
    "> **Note:** The blank model doesn't have sentence detection by default. We need to add the `sentencizer` pipe to enable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4030f47842c670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:30:11.927927590Z",
     "start_time": "2025-12-31T11:30:11.906131742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fe9d74da490>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88343103b40bd081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:30:14.455176738Z",
     "start_time": "2025-12-31T11:30:14.439406864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b9ad901a4a4b8e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:30:57.605951465Z",
     "start_time": "2025-12-31T11:30:57.547939110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion.\n",
      "Hydra is a dragon of India\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion. Hydra is a dragon of India\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
