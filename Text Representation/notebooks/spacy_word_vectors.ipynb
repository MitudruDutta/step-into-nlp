{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862c13dd",
   "metadata": {},
   "source": [
    "# Word Embeddings with spaCy\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Word Embeddings** are dense, low-dimensional vector representations that capture semantic meaning. Unlike BoW/TF-IDF (sparse vectors with thousands of dimensions), embeddings typically have 100-300 dimensions.\n",
    "\n",
    "### Why Embeddings?\n",
    "\n",
    "| Representation | Dimensions | Captures Meaning | Example |\n",
    "|:---------------|:-----------|:-----------------|:--------|\n",
    "| One-Hot | 50,000+ | ‚ùå No | [0,0,1,0,...,0] |\n",
    "| BoW/TF-IDF | 10,000+ | ‚ùå No | [0.2, 0, 0.5,...] |\n",
    "| **Embeddings** | 300 | ‚úÖ Yes | [0.12, -0.34, 0.78,...] |\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Similar words have similar vectors:\n",
    "- \"dog\" and \"cat\" are close in vector space\n",
    "- \"dog\" and \"airplane\" are far apart\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "We need the **large** spaCy model which includes word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a80da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# word vectors occupy lot of space. hence en_core_web_sm model do not have them included. \n",
    "# In order to download\n",
    "# word vectors you need to install large or medium english model. We will install the large one!\n",
    "# make sure you have run \"python -m spacy download en_core_web_lg\" to install large english model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4ec24",
   "metadata": {},
   "source": [
    "### spaCy Model Sizes\n",
    "\n",
    "| Model | Size | Word Vectors | Use Case |\n",
    "|:------|:-----|:-------------|:---------|\n",
    "| `en_core_web_sm` | 12 MB | ‚ùå No | Basic NLP, fast |\n",
    "| `en_core_web_md` | 43 MB | ‚úÖ 20k words | Development |\n",
    "| `en_core_web_lg` | 741 MB | ‚úÖ 685k words | Production |\n",
    "\n",
    "Install large model: `python -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b0ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog Vector: True OOV: False\n",
      "cat Vector: True OOV: False\n",
      "banana Vector: True OOV: False\n",
      "kem Vector: True OOV: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"dog cat banana kem\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"Vector:\", token.has_vector, \"OOV:\", token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e48ac9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Checking Word Vectors\n",
    "\n",
    "Let's examine which words have vectors:\n",
    "- **has_vector**: Does the word have a vector representation?\n",
    "- **is_oov**: Is it Out-Of-Vocabulary? (not in model's training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1213a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5215ab",
   "metadata": {},
   "source": [
    "Notice: \"kem\" is OOV (out of vocabulary) - it's a made-up word with no vector.\n",
    "\n",
    "### Vector Dimensions\n",
    "\n",
    "Each word is represented by a 300-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62cde6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_token = nlp(\"bread\")\n",
    "base_token.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71bc99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Semantic Similarity\n",
    "\n",
    "The magic of embeddings: **similar words have similar vectors**!\n",
    "\n",
    "Let's compare various words to \"bread\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443e1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread <-> bread: 1.0\n",
      "sandwich <-> bread: 0.6874560117721558\n",
      "burger <-> bread: 0.544037401676178\n",
      "car <-> bread: 0.16441147029399872\n",
      "tiger <-> bread: 0.14492356777191162\n",
      "human <-> bread: 0.21103660762310028\n",
      "wheat <-> bread: 0.6572456359863281\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"bread sandwich burger car tiger human wheat\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} <-> {base_token.text}:\", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52098df",
   "metadata": {},
   "source": [
    "**Interpreting Results:**\n",
    "\n",
    "- **sandwich** (0.5+) - Food, closely related to bread\n",
    "- **burger, wheat** - Food-related\n",
    "- **tiger, car** - Unrelated concepts, low similarity\n",
    "\n",
    "The embeddings capture that food items are semantically similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c35619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(base_word, words_to_compare):\n",
    "    base_token = nlp(base_word)\n",
    "    doc = nlp(words_to_compare)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} <-> {base_token.text}: \", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26afdc68",
   "metadata": {},
   "source": [
    "### Helper Function for Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4071a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> iphone:  0.6339781284332275\n",
      "samsung <-> iphone:  0.6678677797317505\n",
      "iphone <-> iphone:  1.0\n",
      "dog <-> iphone:  0.1743103712797165\n",
      "kitten <-> iphone:  0.1468581259250641\n"
     ]
    }
   ],
   "source": [
    "print_similarity(\"iphone\", \"apple samsung iphone dog kitten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd496c4",
   "metadata": {},
   "source": [
    "**Brand awareness**: Notice how \"apple\" and \"samsung\" have high similarity to \"iphone\" - the model learned these are related tech brands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daffd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab[\"king\"].vector\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    "queen = nlp.vocab[\"queen\"].vector\n",
    "\n",
    "result = king - man + woman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5ad9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üëë The Famous King-Queen Analogy\n",
    "\n",
    "The most impressive demonstration of word embeddings:\n",
    "\n",
    "$$\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} \\approx \\vec{\\text{queen}}$$\n",
    "\n",
    "This works because embeddings encode semantic relationships:\n",
    "- \"king\" = royalty + male\n",
    "- \"queen\" = royalty + female\n",
    "- Subtracting \"man\" and adding \"woman\" swaps the gender concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab939b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78808445]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([result], [queen])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26afc4",
   "metadata": {},
   "source": [
    "**Result**: ~0.72 similarity! The analogy works because embeddings capture semantic relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Word Embeddings vs Sparse Representations\n",
    "\n",
    "| Feature | BoW/TF-IDF | Word Embeddings |\n",
    "|:--------|:-----------|:----------------|\n",
    "| Dimensions | 10,000+ | 300 |\n",
    "| Captures meaning | ‚ùå | ‚úÖ |\n",
    "| Similar words | Different vectors | Similar vectors |\n",
    "| Math operations | Meaningless | Semantic! |\n",
    "\n",
    "### When to Use Word Embeddings\n",
    "\n",
    "‚úÖ **Use embeddings for:**\n",
    "- Semantic similarity\n",
    "- Recommendation systems\n",
    "- Transfer learning\n",
    "- Small labeled datasets\n",
    "\n",
    "‚úÖ **Use TF-IDF for:**\n",
    "- Keyword extraction\n",
    "- Search ranking\n",
    "- Large labeled datasets\n",
    "- Interpretability needed\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore the **text_classification.ipynb** to see embeddings in action for fake news detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cf78",
   "metadata": {},
   "source": [
    "### Computing Similarity\n",
    "\n",
    "We use **cosine similarity** to compare vectors:\n",
    "- 1.0 = identical direction\n",
    "- 0.0 = perpendicular (unrelated)\n",
    "- -1.0 = opposite direction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
