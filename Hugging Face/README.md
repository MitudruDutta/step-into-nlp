# Hugging Face Transformers ğŸ¤—

This module covers practical NLP using the **Hugging Face Transformers** library â€” the most popular framework for working with state-of-the-art pretrained language models.

---

## ğŸ“š Table of Contents

1. [Introduction](#introduction)
2. [Topics Covered](#topics-covered)
3. [Key Concepts](#key-concepts)
4. [Prerequisites](#prerequisites)
5. [Getting Started](#getting-started)
6. [Directory Structure](#directory-structure)

---

## Introduction

Hugging Face provides a unified API to access thousands of pretrained models for various NLP tasks. The `transformers` library makes it incredibly easy to:

- Perform **inference** on text using pretrained models
- **Tokenize** text for model consumption
- **Fine-tune** models on custom datasets for specific tasks

This module takes you from using simple pipelines to understanding the underlying components and finally training your own models.

---

## Topics Covered

### 1. Pipelines ğŸš€

ğŸ“– **Documentation:** [docs/pipelines.md](docs/pipelines.md)  
ğŸ““ **Notebook:** [notebooks/pipelines.ipynb](notebooks/pipelines.ipynb)

High-level abstractions for instant NLP capabilities:

- Sentiment Analysis
- Language Translation
- Zero-Shot Classification
- Text Generation
- Named Entity Recognition (NER)

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
classifier("I love Hugging Face!")
```

---

### 2. Tokenizers ğŸ”¤

ğŸ“– **Documentation:** [docs/tokenizers.md](docs/tokenizers.md)  
ğŸ““ **Notebook:** [notebooks/hf_tokenizer.ipynb](notebooks/hf_tokenizer.ipynb)

Understanding how text becomes model input:

- Subword Tokenization (WordPiece, BPE)
- Special Tokens (`[CLS]`, `[SEP]`, `[PAD]`)
- Padding & Truncation strategies
- Batch Processing

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello world", return_tensors="pt")
```

---

### 3. Model Fine-Tuning ğŸ¯

ğŸ“– **Documentation:** [docs/model-finetuning.md](docs/model-finetuning.md)  
ğŸ““ **Notebook:** [notebooks/model_finetuning.ipynb](notebooks/model_finetuning.ipynb)

Training models on your own data:

- Loading & Preprocessing Datasets
- Dynamic Padding with Data Collators
- Training Configuration
- Evaluation Metrics
- Making Predictions

```python
from transformers import Trainer, TrainingArguments
trainer = Trainer(model=model, args=training_args, ...)
trainer.train()
```

---

## Key Concepts

### The Hugging Face Ecosystem

| Component         | Description                                 |
| ----------------- | ------------------------------------------- |
| `transformers`    | Core library for models and tokenizers      |
| `datasets`        | Library for loading and processing datasets |
| `huggingface_hub` | Access to 100,000+ pretrained models        |
| `Trainer`         | High-level API for training and evaluation  |

### Model Architecture Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Text   â”‚ â†’  â”‚  Tokenizer  â”‚ â†’  â”‚   Model     â”‚ â†’  â”‚   Output    â”‚
â”‚             â”‚    â”‚  (encode)   â”‚    â”‚  (forward)  â”‚    â”‚  (logits)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Prerequisites

```bash
pip install transformers datasets torch scikit-learn
```

**Required packages:**

- `transformers` - Hugging Face transformers library
- `datasets` - Dataset loading and processing
- `torch` - PyTorch deep learning framework
- `scikit-learn` - For evaluation metrics
- `numpy` - Numerical operations

---

## Getting Started

### Recommended Learning Path:

```text
1. Pipelines     â†’  Quick results with minimal code
2. Tokenizers    â†’  Understand text preprocessing
3. Fine-Tuning   â†’  Train on your own data
```

Start with the documentation, then practice in the notebooks!

---

## ğŸ“ Directory Structure

```text
Hugging Face/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ pipelines.md             # Detailed pipelines guide
â”‚   â”œâ”€â”€ tokenizers.md            # Detailed tokenizers guide
â”‚   â””â”€â”€ model-finetuning.md      # Detailed fine-tuning guide
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ pipelines.ipynb          # Pipelines tutorial
â”‚   â”œâ”€â”€ hf_tokenizer.ipynb       # Tokenization deep dive
â”‚   â””â”€â”€ model_finetunning.ipynb  # Model fine-tuning guide
â””â”€â”€ results/                     # Training checkpoints
    â”œâ”€â”€ checkpoint-230/
    â”œâ”€â”€ checkpoint-460/
    â””â”€â”€ checkpoint-690/
```

---

## ğŸ”— Resources

- [Hugging Face Documentation](https://huggingface.co/docs/transformers)
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [Hugging Face Course](https://huggingface.co/course)
