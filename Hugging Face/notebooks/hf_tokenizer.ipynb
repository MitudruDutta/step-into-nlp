{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b70be53",
   "metadata": {},
   "source": [
    "# Hugging Face Tokenizers ðŸ”¤\n",
    "\n",
    "Tokenizers are the bridge between human-readable text and the numerical representations that machine learning models understand. This notebook explores how to use Hugging Face tokenizers effectively.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Loading different tokenizers (DistilBERT, BERT)\n",
    "- Understanding tokenization output (input_ids, attention_mask)\n",
    "- Special tokens ([CLS], [SEP], [PAD])\n",
    "- Padding and truncation strategies\n",
    "- Complete inference pipeline from text to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228baca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29049fe",
   "metadata": {},
   "source": [
    "### DistilBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5093fea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138c20376a6d453f9a0f0b55134663aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b65742269f14e9fbb9f49a16d536f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943bb27a4dc4425a8e9295a53a66c0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8404, 3658, 2306, 2017, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"Happiness lies within you\"\n",
    "output = tokenizer(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4449f0",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary with:\n",
    "- **input_ids**: Numerical IDs representing each token\n",
    "- **attention_mask**: 1s for real tokens, 0s for padding\n",
    "\n",
    "Notice the sequence starts with `101` ([CLS]) and ends with `102` ([SEP])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae50451",
   "metadata": {},
   "source": [
    "### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a711630",
   "metadata": {},
   "source": [
    "`AutoTokenizer` is a convenient class that automatically detects and loads the correct tokenizer for any model. This is the recommended approach as it ensures compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7f56fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8404, 3658, 2306, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Happiness lies within you\"\n",
    "\n",
    "output = tokenizer(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed62e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] happiness lies within you [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5eba3",
   "metadata": {},
   "source": [
    "### Decoding: Converting IDs Back to Text\n",
    "\n",
    "The `decode()` method converts token IDs back to human-readable text. Notice how special tokens [CLS] and [SEP] are included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ace455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happiness', 'lies', 'within', 'you', '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c23b3",
   "metadata": {},
   "source": [
    "`convert_ids_to_tokens()` shows the actual token strings, useful for understanding how the tokenizer splits words (subword tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35900e25",
   "metadata": {},
   "source": [
    "### Special token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df033c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc8a30",
   "metadata": {},
   "source": [
    "Each special token has a fixed ID in the vocabulary:\n",
    "- **[CLS]** (101): Classification token, marks sequence start\n",
    "- **[SEP]** (102): Separator token, marks sequence end\n",
    "- **[PAD]** (0): Padding token, fills shorter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922b49bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70096d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747d2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Happiness lies within you\",\n",
    "    \"I love nature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a8975",
   "metadata": {},
   "source": [
    "### Batch Tokenization\n",
    "\n",
    "Tokenizers can process multiple texts at once. This is more efficient than tokenizing one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32beed77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8404, 3658, 2306, 2017, 102], [101, 1045, 2293, 3267, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68093d7c",
   "metadata": {},
   "source": [
    "### padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629c97e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306, 2017,  102],\n",
       "        [ 101, 1045, 2293, 3267,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c8a1f",
   "metadata": {},
   "source": [
    "**`padding=True`**: Pads all sequences to the length of the longest sequence in the batch.\n",
    "\n",
    "**`return_tensors='pt'`**: Returns PyTorch tensors instead of Python lists (required for model input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c76c5136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306,  102],\n",
       "        [ 101, 1045, 2293, 3267,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding='max_length', max_length=5, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50afb9",
   "metadata": {},
   "source": [
    "**`padding='max_length'`** + **`max_length=5`**: Forces all sequences to exactly 5 tokens.\n",
    "\n",
    "**`truncation=True`**: Cuts sequences longer than max_length. Here, our texts get truncated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adffec6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306, 2017,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1045, 2293, 3267,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding='max_length', max_length=20, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825df23",
   "metadata": {},
   "source": [
    "With `max_length=20`, sequences are padded (not truncated) to reach 20 tokens. The attention_mask shows which tokens are real (1) vs padding (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e00e5f",
   "metadata": {},
   "source": [
    "### Supplying tokens to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 19:15:37.287468: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-02 19:15:37.325949: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-02 19:15:38.954299: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-02 19:15:38.954555: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/home/btwitsvoid/python/lib64/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f673118105a64f62a2a4abedbd31cbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0561, -3.2456],\n",
       "        [-3.6340,  3.8584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"That phone case broke after 2 days of use\", \n",
    "    \"That herbal tea has helped me so much\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553543d",
   "metadata": {},
   "source": [
    "Now let's see the complete workflow: tokenize text, pass to model, and get predictions. We use a pre-trained sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "474013bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9933e-01, 6.7395e-04],\n",
       "        [5.5700e-04, 9.9944e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(output.logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56baf69",
   "metadata": {},
   "source": [
    "### Post-Processing: Logits to Probabilities\n",
    "\n",
    "The model outputs raw **logits** (unnormalized scores). We apply **softmax** to convert them to probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71676b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(probs, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b601c20",
   "metadata": {},
   "source": [
    "`argmax` selects the class with highest probability:\n",
    "- **0** = NEGATIVE\n",
    "- **1** = POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdfe1c",
   "metadata": {},
   "source": [
    "input text ==> tokenizer ==> tokens(token ids) ==> model ==> logits ==> post processing ==> output text\n",
    "\n",
    "Previously when we used HuggingFace pipeline we were able to do all of this with just one line of code. Above code explains the inner workings of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331345bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997941851615906}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "pipe(\"My dog is cute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142ab4e",
   "metadata": {},
   "source": [
    "### The Pipeline Shortcut\n",
    "\n",
    "The **pipeline** does ALL of the above (tokenization â†’ model â†’ post-processing) in one line! It's the easiest way to use transformers for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
